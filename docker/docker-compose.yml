services:
  # ðŸŒ Nginx reverse proxy - SSL/HTTPS configuration
  nginx:
    image: nginx:alpine
    container_name: core-nginx
    ports:
      - "${HTTP_PORT:-80}:80"
      - "${HTTPS_PORT:-443}:443"
    environment:
      - DOMAIN=${DOMAIN:-core-platform.local}
    volumes:
      - ./nginx/nginx-ssl.conf.template:/etc/nginx/templates/nginx-ssl.conf.template:ro
      - ./nginx/start-nginx.sh:/usr/local/bin/start-nginx.sh:ro
      # ðŸ”’ Updated to use new wildcard SSL certificates
      - ../ssl:/etc/nginx/ssl:ro
      - certbot-www:/var/www/certbot:ro
    command: [ "/usr/local/bin/start-nginx.sh" ]
    depends_on:
      frontend:
        condition: service_healthy  # âœ… OPRAVENO: ÄŒekÃ¡ na frontend health check
      backend:
        condition: service_started
      keycloak:
        condition: service_started
      grafana:
        condition: service_started
    networks:
      - core-net
    restart: unless-stopped

  # ðŸ—„ï¸ Database
  db:
    image: postgres:16
    container_name: core-db
    environment:
      POSTGRES_USER: ${DB_INTERNAL_USERNAME}
      POSTGRES_PASSWORD: ${DB_INTERNAL_PASSWORD}
      POSTGRES_DB: ${DB_INTERNAL_NAME}
      # Keycloak database credentials pro init skripty
      KEYCLOAK_DB_USERNAME: ${KEYCLOAK_DB_USERNAME}
      KEYCLOAK_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD}
      KEYCLOAK_DB_NAME: ${KEYCLOAK_DB_NAME}
    volumes:
      - core_db_data:/var/lib/postgresql/data
      - ./db/init:/docker-entrypoint-initdb.d:ro
    ports:
      - "${DB_PORT:-5432}:5432"
    networks:
      - core-net
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # ðŸ—„ï¸ PgAdmin
  pgadmin:
    image: dpage/pgadmin4
    container_name: core-pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
    ports:
      - "5050:80"
    networks:
      - core-net

  # ðŸ—ï¸ Backend
  backend:
    build:
      context: ..
      dockerfile: docker/backend/Dockerfile
    container_name: core-backend
    depends_on:
      db:
        condition: service_healthy
      keycloak:
        condition: service_started
      loki:
        condition: service_started
    environment:
      - ENVIRONMENT=${ENVIRONMENT:-development}
      # ðŸŒ DOMAIN pro vÅ¡echny external URLs
      - DOMAIN=${DOMAIN:-core-platform.local}
      # ðŸ”§ FIX: OIDC Configuration - opravenÃ© issuery pro JWT validaci
      - OIDC_ISSUER_URI=https://admin.${DOMAIN:-core-platform.local}/realms/admin
      - OIDC_JWK_SET_URI=https://keycloak:8443/realms/admin/protocol/openid-connect/certs
      - OIDC_API_AUDIENCE=api
      # ðŸ”§ NEW: Internal Keycloak URL for JWT decoder to download JWK keys via Docker network
      - KEYCLOAK_INTERNAL_BASE_URL=https://keycloak:8443
      # ðŸ”§ FIX: CORS origins - wildcard pro vÅ¡echny subdomÃ©ny
      - CORS_ORIGINS=${CORS_ORIGINS}
      - DATABASE_URL=${DATABASE_URL:-jdbc:postgresql://db:5432/core}
      - LOKI_URL=http://loki:3100/loki/api/v1/push
      - LOG_LEVEL=DEBUG
      - SPRING_PROFILES_ACTIVE=development
      # ðŸ” Service Account Credentials
      - KEYCLOAK_ADMIN_CLIENT_ID=backend-admin-service
      - KEYCLOAK_ADMIN_CLIENT_SECRET=${KEYCLOAK_ADMIN_CLIENT_SECRET}
      - KEYCLOAK_CLIENT_SECRET=${KEYCLOAK_CLIENT_SECRET:-}
      - KEYCLOAK_ADMIN_REALM=${KEYCLOAK_ADMIN_REALM}
      - KEYCLOAK_TARGET_REALM=${KEYCLOAK_TARGET_REALM}
      - KEYCLOAK_ADMIN_BASE_URL=http://keycloak:8080
      # ðŸ¢ MULTITENANCY Configuration
      - TENANCY_DEFAULT_TENANT_KEY=${TENANCY_DEFAULT_TENANT_KEY:-admin}
      - AUTH_JWT_TENANT_CLAIM=${AUTH_JWT_TENANT_CLAIM:-tenant}
      # ðŸ“… KEYCLOAK BACKFILL Configuration
      - APP_KEYCLOAK_BACKFILL_ENABLED=${APP_KEYCLOAK_BACKFILL_ENABLED:-true}
      - APP_KEYCLOAK_BACKFILL_CRON=${APP_KEYCLOAK_BACKFILL_CRON:-0 25 3 * * *}
      - KEYCLOAK_ADMIN_USERNAME=${KEYCLOAK_ADMIN_USERNAME:-admin}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD:-admin123}  # ðŸ”§ FIX: PÅ™idÃ¡no "123" pro konzistenci s .env
      # ðŸ”’ SSL Configuration for HTTPS communication with Keycloak
      - JAVA_OPTS=-Djavax.net.ssl.trustStore=/etc/ssl/certs/java/cacerts -Djavax.net.ssl.trustStorePassword=changeit -Djavax.net.ssl.trustStoreType=JKS
      # ðŸ“Š Streaming & Kafka Configuration
      - STREAMING_ENABLED=${STREAMING_ENABLED:-false}
      - KAFKA_SERVERS=${KAFKA_SERVERS:-kafka:9092}
      - STREAMING_TOPIC_PREFIX=${STREAMING_TOPIC_PREFIX:-core}
      - STREAMING_SECURITY_MODE=${STREAMING_SECURITY_MODE:-PLAINTEXT}
      - PROMETHEUS_PORT=${PROMETHEUS_PORT:-9090}
      - GRAFANA_PUBLIC_URL=${GRAFANA_PUBLIC_URL:-https://grafana.${DOMAIN}}
      # ðŸ“Š Grafana SSO Integration
      - GRAFANA_JWT_TTL=${GRAFANA_JWT_TTL:-300}  # Must match Keycloak access token lifespan (5 min)
      - GRAFANA_JWT_SECRET=${GRAFANA_JWT_SECRET:-change-me-in-production-to-secure-random-256bit-key}
      # ðŸ“Š Loki Configuration (aplikaÄnÃ­ logging pÅ™es Logback)
      - LOKI_URL=http://core-loki:3100/loki/api/v1/push
    volumes:
      # ðŸ”’ Updated to use new wildcard SSL certificates
      - ../ssl:/etc/ssl/backend:ro
    # âœ… Docker logging driver VYPNUT - pouÅ¾Ã­vÃ¡me aplikaÄnÃ­ logging (Logback + Loki4j)
    # VÃ½hody:
    # - Funguje v K8s/cloud (nevÃ¡Å¾e se na localhost)
    # - LepÅ¡Ã­ strukturovanÃ© logy (JSON s MDC kontextem)
    # - AsynchronnÃ­ batching (neblokuje aplikaci)
    # - Tenant context v kaÅ¾dÃ©m logu
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
    networks:
      - core-net
    restart: unless-stopped

  # ðŸ›¡ï¸ Frontend - PRODUKÄŒNÃ BUILD PRO DEV PARITY
  frontend:
    build:
      context: ..
      dockerfile: docker/frontend/Dockerfile
      args:
        STREAMING_ENABLED: ${STREAMING_ENABLED:-false}
        GRAFANA_PUBLIC_URL: ${GRAFANA_PUBLIC_URL:-https://grafana.${DOMAIN:-core-platform.local}}
        API_BASE_URL: /api
    container_name: core-frontend
    # NOTE: No volume mount - static files are baked into the image during build
    # For development with hot-reload, use: make dev-watch or docker compose watch
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost/health || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      backend:
        condition: service_started
      loki:
        condition: service_started
    # âœ… Docker logging driver VYPNUT - frontend posÃ­lÃ¡ logy pÅ™es HTTP API
    # Frontend mÃ¡ vlastnÃ­ logger service kterÃ½ posÃ­lÃ¡ logy do BE API endpoint
    # Backend pak forwarduje do Loki s tenant kontextem
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    networks:
      - core-net
    restart: unless-stopped

  # ðŸ” Keycloak
  keycloak:
    build:
      context: ..
      dockerfile: docker/keycloak/Dockerfile
    image: core-platform/keycloak:local
    container_name: core-keycloak
    restart: unless-stopped
    ports:
      - "8081:8443" # ðŸ”’ HTTPS port mapping
    environment:
      # ðŸ” Admin credentials
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin123}
      
      # ðŸ—„ï¸ Database Configuration - pouÅ¾Ã­vÃ¡ env promÄ›nnÃ© mÃ­sto hardcoded
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://db:5432/${KEYCLOAK_DB_NAME}
      KC_DB_USERNAME: ${KEYCLOAK_DB_USERNAME}
      KC_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD}
      
      # ðŸ‘¥ Test User Credentials (DEVELOPMENT ONLY)
      TEST_USER_PASSWORD: ${TEST_USER_PASSWORD:-Test.1234}
      TEST_ADMIN_PASSWORD: ${TEST_ADMIN_PASSWORD:-Test.1234}

      # ðŸ”’ HTTPS Production Mode Configuration - OPRAVENO: dynamickÃ© hostname detection
      KC_PROXY: "edge"
      KC_HOSTNAME_STRICT: "false"
      KC_HOSTNAME_STRICT_HTTPS: "false"
      KC_HTTP_ENABLED: "false" # ðŸ”’ Disable HTTP
      KC_HTTPS_PORT: "8443" # ðŸ”’ HTTPS port
      KC_HTTPS_CLIENT_AUTH: "none"
      # ðŸ”’ SSL Certificate configuration
      KC_HTTPS_CERTIFICATE_FILE: /etc/ssl/keycloak/cert.pem
      KC_HTTPS_CERTIFICATE_KEY_FILE: /etc/ssl/keycloak/key.pem

      # ðŸ”— Event listeners configuration - pouze jboss-logging
      KC_EVENTS_LISTENERS: "jboss-logging"

      # ðŸ“Š Logging and health
      KC_LOG_LEVEL: ${KC_LOG_LEVEL:-INFO}
      KC_HEALTH_ENABLED: "true"

    volumes:
      - keycloak_data:/opt/keycloak/data
      # ðŸ”’ Updated to use new wildcard SSL certificates
      - ../ssl:/etc/ssl/keycloak:ro

    # ðŸ”’ Production HTTPS mode - removes dev mode
    # Note: CMD is defined in Dockerfile now

    networks:
      - core-net

  # ðŸ—ï¸ VolitelnÃ¡ bootstrap sluÅ¾ba pro automatickÃ© nastavenÃ­ tenant realmÅ¯
  kc-bootstrap:
    image: core-platform/keycloak:local
    container_name: core-kc-bootstrap
    profiles:
      - bootstrap # Aktivuje se pouze s --profile bootstrap
    depends_on:
      keycloak:
        condition: service_started
    environment:
      # ðŸ“‹ Bootstrap parametry
      - REALM=${AUTO_BOOTSTRAP_REALM:-}
      - TENANT_ADMIN=${AUTO_BOOTSTRAP_TENANT_ADMIN:-tenant-admin}
      - TENANT_ADMIN_PASSWORD=${AUTO_BOOTSTRAP_TENANT_ADMIN_PASSWORD:-}
      - KEYCLOAK_ADMIN_USER=${KEYCLOAK_ADMIN:-admin}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD:-admin123}
      - KC_BASE_URL=https://keycloak:8443
    volumes:
      - ../scripts:/scripts:ro
    command: >
      sh -c "
        if [ -z \"$$REALM\" ]; then
          echo 'âš ï¸  AUTO_BOOTSTRAP_REALM not set, skipping bootstrap';
          exit 0;
        fi;
        echo 'ðŸ—ï¸ Auto-bootstrapping realm: '$$REALM;
        echo 'â³ Waiting for Keycloak to be ready...';
        for i in \$$(seq 1 120); do
          if wget -q --spider https://keycloak:8443/health/ready 2>/dev/null; then
            break;
          fi;
          sleep 2;
        done;
        echo 'âœ… Keycloak is ready, starting bootstrap...';
        /scripts/kc_bootstrap_realm.sh;
        echo 'ðŸŽ‰ Bootstrap completed, container will exit';
      "
    networks:
      - core-net
    restart: "no" # SpustÃ­ se pouze jednou

  # ðŸ“Š Monitoring Stack
  # ðŸ”§ Grafana Tenant Provisioning Service
  grafana-provisioner:
    image: postgres:16
    container_name: core-grafana-provisioner
    environment:
      - GRAFANA_URL=http://grafana:3000
      - GRAFANA_ADMIN_USER=admin
      - GRAFANA_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=${DB_INTERNAL_NAME:-core}
      - DB_USER=${DB_INTERNAL_USERNAME:-core}
      - DB_PASSWORD=${DB_INTERNAL_PASSWORD:-core}
      # â„¹ï¸ TENANTS is now loaded dynamically from database (tenants table)
      # - TENANTS=admin test-tenant company-b  # DEPRECATED
    volumes:
      - ./grafana/provision-tenants.sh:/tmp/provision-tenants.sh:ro
    command: >
      sh -c "
        echo 'â³ Installing dependencies (curl, jq)...';
        apt-get update -qq && apt-get install -y -qq curl jq > /dev/null 2>&1;
        echo 'âœ… Dependencies installed';
        cp /tmp/provision-tenants.sh /usr/local/bin/provision-tenants.sh;
        chmod +x /usr/local/bin/provision-tenants.sh;
        /usr/local/bin/provision-tenants.sh;
        echo 'ðŸŽ‰ Provisioning completed, container will exit';
      "
    depends_on:
      backend:
        condition: service_healthy
      grafana:
        condition: service_started
      db:
        condition: service_healthy
    networks:
      - core-net
    restart: "no" # SpustÃ­ se pouze jednou

  grafana:
    build:
      context: .
      dockerfile: grafana/Dockerfile
    image: core-grafana:custom
    container_name: core-grafana
    environment:
      # ðŸ—„ï¸ PostgreSQL Database (instead of SQLite for persistence across restarts)
      # Dedicated grafana user/database (same pattern as keycloak)
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=db:5432
      - GF_DATABASE_NAME=${GRAFANA_DB_NAME}
      - GF_DATABASE_USER=${GRAFANA_DB_USERNAME}
      - GF_DATABASE_PASSWORD=${GRAFANA_DB_PASSWORD}
      - GF_DATABASE_SSL_MODE=disable
      
      # ï¿½ðŸ” JWT Authentication (Primary - for iframe embedding)
      - GF_AUTH_JWT_ENABLED=true
      - GF_AUTH_JWT_HEADER_NAME=X-Org-JWT
      - GF_AUTH_JWT_USERNAME_CLAIM=preferred_username
      - GF_AUTH_JWT_EMAIL_CLAIM=email
      # ðŸ”‘ JWT Verification - Use BFF's public key (RS256)
      # BFF mints short-lived JWT (300s = 5 min) with orgId claim
      # Grafana verifies signature via BFF JWKS endpoint (internal HTTP)
      # This solves multi-realm problem: one JWKS for all tenants
      - GF_AUTH_JWT_JWK_SET_URL=http://backend:8080/.well-known/jwks.json
      - GF_AUTH_JWT_URL_LOGIN=false
      # âš ï¸ CRITICAL: JWT TTL and cache must align (both 300s = 5 min)
      # - Backend: ${GRAFANA_JWT_TTL} (from .env, default 300)
      # - Grafana cache: GF_AUTH_JWT_CACHE_TTL (5m = 300s)
      - GF_AUTH_JWT_CACHE_TTL=${GRAFANA_JWT_CACHE_TTL:-5m}
      - GF_AUTH_JWT_AUTO_SIGN_UP=true
      # ðŸŽ­ Role mapping based on Keycloak roles
      - GF_AUTH_JWT_ROLE_ATTRIBUTE_PATH=contains(realm_access.roles[*], 'CORE_ROLE_ADMIN') && 'Admin' || contains(realm_access.roles[*], 'CORE_ROLE_MONITORING') && 'Editor' || contains(realm_access.roles[*], 'CORE_TENANT_ADMIN') && 'Editor' || contains(realm_access.roles[*], 'CORE_ROLE_TENANT_MONITORING') && 'Viewer' || 'Viewer'
      - GF_AUTH_JWT_SKIP_ORG_ROLE_SYNC=false
      # ðŸ¢ CRITICAL: Org ID claim - maps JWT orgId claim to Grafana organization
      - GF_AUTH_JWT_ORG_ID_CLAIM=orgId
      
      # ðŸ”“ DÅ®LEÅ½ITÃ‰: Povolit login form pro fallback + admin pÅ™Ã­stup
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      
      # ðŸ” OAuth2 / Keycloak SSO Configuration (for direct login + SSO)
      - GF_AUTH_GENERIC_OAUTH_ENABLED=true
      - GF_AUTH_GENERIC_OAUTH_NAME=Keycloak
      - GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP=true
      - GF_AUTH_GENERIC_OAUTH_CLIENT_ID=grafana
      - GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=${GRAFANA_OAUTH_SECRET:-grafana-secret-change-in-prod}
      - GF_AUTH_GENERIC_OAUTH_SCOPES=openid profile email
      - GF_AUTH_GENERIC_OAUTH_AUTH_URL=https://admin.${DOMAIN:-core-platform.local}/realms/admin/protocol/openid-connect/auth
      - GF_AUTH_GENERIC_OAUTH_TOKEN_URL=https://keycloak:8443/realms/admin/protocol/openid-connect/token
      - GF_AUTH_GENERIC_OAUTH_API_URL=https://keycloak:8443/realms/admin/protocol/openid-connect/userinfo
      - GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH=contains(realm_access.roles[*], 'CORE_ROLE_ADMIN') && 'Admin' || contains(realm_access.roles[*], 'CORE_ROLE_MONITORING') && 'Editor' || contains(realm_access.roles[*], 'CORE_TENANT_ADMIN') && 'Editor' || contains(realm_access.roles[*], 'CORE_ROLE_TENANT_MONITORING') && 'Viewer' || 'Viewer'
      - GF_AUTH_GENERIC_OAUTH_TLS_SKIP_VERIFY_INSECURE=true
      - GF_AUTH_GENERIC_OAUTH_USE_PKCE=true
      
      # ðŸ”’ Security settings
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_PROTOCOL=http
      # CRITICAL: Must include trailing slash AND be relative for multi-tenant
      # DO NOT hardcode domain - Grafana will read from Host header
      - GF_SERVER_HTTP_PORT=3000
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_SERVER_ROOT_URL=/core-admin/monitoring/
      
      # ðŸŽ¨ UI Configuration
      - GF_AUTH_OAUTH_AUTO_LOGIN=false
      - GF_AUTH_OAUTH_ALLOW_INSECURE_EMAIL_LOOKUP=true
      
      # ðŸ”„ Redirect configuration - relative path for multi-tenant support
      # Browser will use current domain (admin.core-platform.local or ten.core-platform.local)
      - GF_AUTH_SIGNOUT_REDIRECT_URL=/realms/admin/protocol/openid-connect/logout?redirect_uri=/core-admin/monitoring/
      
      # ðŸ“‚ Provisioning paths
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/jwt-secret.key:/etc/grafana/jwt-secret.key:ro
      # DISABLED: grafana.ini mount - using env vars only
      # - ./grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    ports:
      - "3001:3000"  # Temporary for datasource setup
    networks:
      - core-net
    depends_on:
      db:
        condition: service_healthy
      keycloak:
        condition: service_started
      loki:
        condition: service_started

  loki:
    image: grafana/loki:3.0.0
    container_name: core-loki
    ports:
      - "${LOKI_PORT:-3100}:3100"
    command: -config.file=/etc/loki/config.yml
    volumes:
      - ./loki/config.yml:/etc/loki/config.yml
      - loki_data:/loki
    networks:
      - core-net

  promtail:
    image: grafana/promtail:3.0.0
    container_name: core-promtail
    command: -config.file=/etc/promtail/config.yml
    ports:
      - "9080:9080"
    volumes:
      - ./promtail/config.yml:/etc/promtail/config.yml
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail_positions:/tmp
    depends_on:
      - loki
    networks:
      - core-net
    user: "0:0"

  prometheus:
    image: prom/prometheus:v2.54.0
    container_name: core-prometheus
    ports:
      - "9091:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
      - ./prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    depends_on:
      - loki
      - promtail
    networks:
      - core-net

  node-exporter:
    image: prom/node-exporter:v1.8.1
    container_name: core-node-exporter
    ports:
      - "9100:9100"
    command:
      - --path.procfs=/host/proc
      - --path.sysfs=/host/sys
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
    networks:
      - core-net
    profiles:
      - vm

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.2
    container_name: core-cadvisor
    ports:
      - "8082:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - core-net
    privileged: true

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: core-postgres-exporter
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://${DB_INTERNAL_USERNAME}:${DB_INTERNAL_PASSWORD}@db:5432/${DB_INTERNAL_NAME}?sslmode=disable
    depends_on:
      - db
    networks:
      - core-net

  # ðŸ”´ Redis for caching and presence (Phase 2)
  redis:
    image: redis:7-alpine
    container_name: core-redis
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - core-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # ï¿½ Kafka (KRaft mode - no Zookeeper needed)
  kafka:
    image: apache/kafka:3.8.1
    container_name: core-kafka
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      # KRaft mode configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      CLUSTER_ID: 5L6g3nShT-eMCtK--X86sw
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      # Performance & Storage settings
      KAFKA_HEAP_OPTS: "-Xmx768m -Xms768m"
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      # Topic auto-creation disabled - we use TopicEnsurer
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      # Metrics for Prometheus
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - core-net
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles:
      - streaming

  # ðŸŽ›ï¸ Kafka UI for management and monitoring
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: core-kafka-ui
    ports:
      - "8090:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: core-platform
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_METRICS_PORT: 9999
      DYNAMIC_CONFIG_ENABLED: "true"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - core-net
    profiles:
      - streaming

  # ï¿½ðŸ“¦ MinIO for document storage (Phase 2)
  minio:
    image: minio/minio:latest
    container_name: core-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ACCESS_KEY:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_SECRET_KEY:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    networks:
      - core-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # ðŸ“Š Cube.js - Semantic Layer for Reporting
  cube:
    image: cubejs/cube:latest
    container_name: core-cube
    ports:
      - "${CUBE_PORT:-4000}:4000"
    environment:
      - CUBEJS_DB_TYPE=postgres
      - CUBEJS_DB_HOST=db
      - CUBEJS_DB_PORT=5432
      - CUBEJS_DB_NAME=${DB_INTERNAL_NAME:-core}
      - CUBEJS_DB_USER=${DB_INTERNAL_USERNAME:-core}
      - CUBEJS_DB_PASS=${DB_INTERNAL_PASSWORD:-core}
      - CUBEJS_API_SECRET=${CUBE_API_SECRET:-dev_secret_change_in_production}
      - CUBEJS_DEV_MODE=${CUBE_DEV_MODE:-true}
      - CUBEJS_WEB_SOCKETS=true
      - CUBEJS_REDIS_URL=redis://redis:6379
      - CUBEJS_CACHE_AND_QUEUE_DRIVER=cubestore
      - NODE_ENV=${ENVIRONMENT:-development}
    volumes:
      - ./cube/schema:/cube/conf/schema:ro
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - core-net
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:4000/readyz', (r) => process.exit(r.statusCode === 200 ? 0 : 1))"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped

volumes:
  core_db_data:
  grafana_data:
  loki_data:
  promtail_positions:
  prometheus_data:
  certbot-www:
  keycloak_data:
  redis_data:
  minio_data:
  kafka_data:


networks:
  core-net:
    driver: bridge
