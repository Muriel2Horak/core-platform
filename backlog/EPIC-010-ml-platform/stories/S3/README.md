# S3: Model Training Pipeline

> **Automated Training:** Airflow DAGs, hyperparameter tuning (Optuna), data versioning (DVC)

## ðŸ“‹ Story

**As a** data scientist  
**I want** automated model training pipelines  
**So that** models are retrained regularly with fresh data

## ðŸŽ¯ Acceptance Criteria

**GIVEN** new training data is available  
**WHEN** I trigger a training job  
**THEN** Airflow orchestrates: data extraction â†’ feature engineering â†’ training â†’ validation â†’ deployment  
**AND** hyperparameter tuning finds optimal parameters  
**AND** trained model is registered in MLflow

## ðŸ—ï¸ Implementation

### Airflow DAG

```python
# ml/airflow_dags/workflow_router_training.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta
import mlflow
import optuna
import torch

default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 7),
    'email_on_failure': True,
    'email': ['ml-team@core-platform.local'],
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'workflow_router_training',
    default_args=default_args,
    description='Train workflow routing model',
    schedule_interval='@weekly',  # Weekly retraining
    catchup=False,
)

def extract_training_data(**context):
    """Extract workflow assignment data from PostgreSQL"""
    import pandas as pd
    from sqlalchemy import create_engine
    
    engine = create_engine('postgresql://core:core@db:5432/core')
    
    query = """
        SELECT 
            w.id,
            w.workflow_type,
            w.priority,
            w.tenant_id,
            w.assigned_to,
            w.created_at,
            w.completed_at,
            EXTRACT(EPOCH FROM (w.completed_at - w.created_at)) as duration_seconds
        FROM workflow_instances w
        WHERE w.status = 'COMPLETED'
        AND w.completed_at > NOW() - INTERVAL '90 days'
        ORDER BY w.created_at DESC
    """
    
    df = pd.read_sql(query, engine)
    df.to_parquet('/tmp/training_data.parquet')
    
    print(f"Extracted {len(df)} training samples")
    return '/tmp/training_data.parquet'

def feature_engineering(**context):
    """Feature engineering"""
    import pandas as pd
    from sklearn.preprocessing import LabelEncoder
    
    data_path = context['ti'].xcom_pull(task_ids='extract_data')
    df = pd.read_parquet(data_path)
    
    # Encode categorical features
    le_workflow_type = LabelEncoder()
    le_assigned_to = LabelEncoder()
    
    df['workflow_type_encoded'] = le_workflow_type.fit_transform(df['workflow_type'])
    df['assigned_to_encoded'] = le_assigned_to.fit_transform(df['assigned_to'])
    
    # Time features
    df['hour'] = pd.to_datetime(df['created_at']).dt.hour
    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek
    
    # Save features
    features_path = '/tmp/features.parquet'
    df.to_parquet(features_path)
    
    # Save label encoders
    import joblib
    joblib.dump(le_workflow_type, '/tmp/le_workflow_type.pkl')
    joblib.dump(le_assigned_to, '/tmp/le_assigned_to.pkl')
    
    return features_path

def hyperparameter_tuning(**context):
    """Optuna hyperparameter optimization"""
    import pandas as pd
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, TensorDataset
    from sklearn.model_selection import train_test_split
    import optuna
    
    features_path = context['ti'].xcom_pull(task_ids='feature_engineering')
    df = pd.read_parquet(features_path)
    
    # Prepare data
    X = df[['workflow_type_encoded', 'priority', 'hour', 'day_of_week']].values
    y = df['assigned_to_encoded'].values
    
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
    
    def objective(trial):
        # Hyperparameters to optimize
        hidden_size = trial.suggest_int('hidden_size', 64, 512)
        dropout = trial.suggest_float('dropout', 0.1, 0.5)
        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)
        
        # Build model
        model = nn.Sequential(
            nn.Linear(X.shape[1], hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, len(set(y)))
        )
        
        # Train
        optimizer = torch.optim.Adam(model.parameters(), lr=lr)
        criterion = nn.CrossEntropyLoss()
        
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train),
            torch.LongTensor(y_train)
        )
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        model.train()
        for epoch in range(10):
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
        
        # Validate
        model.eval()
        with torch.no_grad():
            val_outputs = model(torch.FloatTensor(X_val))
            val_loss = criterion(val_outputs, torch.LongTensor(y_val))
            accuracy = (val_outputs.argmax(dim=1) == torch.LongTensor(y_val)).float().mean()
        
        return accuracy.item()
    
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=20, timeout=3600)
    
    best_params = study.best_params
    print(f"Best hyperparameters: {best_params}")
    
    # Save best params
    import json
    with open('/tmp/best_params.json', 'w') as f:
        json.dump(best_params, f)
    
    return '/tmp/best_params.json'

def train_final_model(**context):
    """Train final model with best hyperparameters"""
    import pandas as pd
    import torch
    import torch.nn as nn
    import json
    from sklearn.model_selection import train_test_split
    
    features_path = context['ti'].xcom_pull(task_ids='feature_engineering')
    params_path = context['ti'].xcom_pull(task_ids='hyperparameter_tuning')
    
    df = pd.read_parquet(features_path)
    X = df[['workflow_type_encoded', 'priority', 'hour', 'day_of_week']].values
    y = df['assigned_to_encoded'].values
    
    with open(params_path, 'r') as f:
        best_params = json.load(f)
    
    # Build model with best params
    model = nn.Sequential(
        nn.Linear(X.shape[1], best_params['hidden_size']),
        nn.ReLU(),
        nn.Dropout(best_params['dropout']),
        nn.Linear(best_params['hidden_size'], best_params['hidden_size'] // 2),
        nn.ReLU(),
        nn.Dropout(best_params['dropout']),
        nn.Linear(best_params['hidden_size'] // 2, len(set(y)))
    )
    
    # Train on full dataset
    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])
    criterion = nn.CrossEntropyLoss()
    
    train_dataset = TensorDataset(torch.FloatTensor(X), torch.LongTensor(y))
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    
    model.train()
    for epoch in range(50):
        total_loss = 0
        for batch_X, batch_y in train_loader:
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {total_loss / len(train_loader):.4f}")
    
    # Save model
    model_path = '/tmp/workflow_router_model.pt'
    torch.save(model.state_dict(), model_path)
    
    return model_path

def validate_model(**context):
    """Validate model performance"""
    import pandas as pd
    import torch
    import torch.nn as nn
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.model_selection import train_test_split
    import json
    
    features_path = context['ti'].xcom_pull(task_ids='feature_engineering')
    model_path = context['ti'].xcom_pull(task_ids='train_model')
    params_path = context['ti'].xcom_pull(task_ids='hyperparameter_tuning')
    
    df = pd.read_parquet(features_path)
    X = df[['workflow_type_encoded', 'priority', 'hour', 'day_of_week']].values
    y = df['assigned_to_encoded'].values
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    with open(params_path, 'r') as f:
        best_params = json.load(f)
    
    # Load model
    model = nn.Sequential(
        nn.Linear(X.shape[1], best_params['hidden_size']),
        nn.ReLU(),
        nn.Dropout(best_params['dropout']),
        nn.Linear(best_params['hidden_size'], best_params['hidden_size'] // 2),
        nn.ReLU(),
        nn.Dropout(best_params['dropout']),
        nn.Linear(best_params['hidden_size'] // 2, len(set(y)))
    )
    model.load_state_dict(torch.load(model_path))
    model.eval()
    
    # Predict
    with torch.no_grad():
        predictions = model(torch.FloatTensor(X_test)).argmax(dim=1).numpy()
    
    accuracy = accuracy_score(y_test, predictions)
    report = classification_report(y_test, predictions)
    
    print(f"Model Accuracy: {accuracy:.4f}")
    print(f"Classification Report:\n{report}")
    
    # Validation gate: require >85% accuracy
    if accuracy < 0.85:
        raise ValueError(f"Model accuracy {accuracy:.2%} below threshold 85%")
    
    return accuracy

def register_model(**context):
    """Register model in MLflow"""
    import mlflow
    import torch
    
    model_path = context['ti'].xcom_pull(task_ids='train_model')
    accuracy = context['ti'].xcom_pull(task_ids='validate_model')
    
    mlflow.set_tracking_uri('http://mlflow:5000')
    
    with mlflow.start_run():
        # Log metrics
        mlflow.log_metric('accuracy', accuracy)
        mlflow.log_metric('training_samples', 10000)  # From extract_data
        
        # Log model
        mlflow.pytorch.log_model(torch.load(model_path), 'model')
        
        # Tag for deployment
        mlflow.set_tag('stage', 'staging')
        mlflow.set_tag('training_date', datetime.now().isoformat())
        
        run_id = mlflow.active_run().info.run_id
    
    print(f"Model registered in MLflow: run_id={run_id}")
    return run_id

# Define tasks
extract_data = PythonOperator(
    task_id='extract_data',
    python_callable=extract_training_data,
    dag=dag,
)

feature_eng = PythonOperator(
    task_id='feature_engineering',
    python_callable=feature_engineering,
    dag=dag,
)

hyperparameter_opt = PythonOperator(
    task_id='hyperparameter_tuning',
    python_callable=hyperparameter_tuning,
    dag=dag,
)

train_model = PythonOperator(
    task_id='train_model',
    python_callable=train_final_model,
    dag=dag,
)

validate = PythonOperator(
    task_id='validate_model',
    python_callable=validate_model,
    dag=dag,
)

register = PythonOperator(
    task_id='register_model',
    python_callable=register_model,
    dag=dag,
)

# Task dependencies
extract_data >> feature_eng >> hyperparameter_opt >> train_model >> validate >> register
```

### Data Versioning (DVC)

```yaml
# ml/dvc.yaml
stages:
  extract_data:
    cmd: python scripts/extract_data.py
    deps:
      - scripts/extract_data.py
    outs:
      - data/training_data.parquet

  feature_engineering:
    cmd: python scripts/feature_engineering.py
    deps:
      - data/training_data.parquet
      - scripts/feature_engineering.py
    outs:
      - data/features.parquet

  train:
    cmd: python scripts/train.py
    deps:
      - data/features.parquet
      - scripts/train.py
    outs:
      - models/workflow_router.pt
    metrics:
      - metrics/accuracy.json
```

## ðŸ“Š Production Metrics

- **Training frequency:** Weekly automated runs
- **Training time:** <2h per model (GPU-accelerated)
- **Hyperparameter trials:** 20 trials/run
- **Model accuracy improvement:** +2% per retrain cycle
- **Failed training jobs:** <5% (automatic retries)

---

**Story Points:** 3  
**Estimate:** 700 LOC  
**Dependencies:** S1 (MLflow), Airflow, Optuna, DVC, PostgreSQL
