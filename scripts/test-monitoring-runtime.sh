#!/bin/bash
# ğŸ­ POST-DEPLOY MONITORING RUNTIME TESTS
# Tests actual monitoring behavior in running environment
# Simulates real scenarios and verifies alerts fire correctly
# Exit code 0 = success, non-zero = failure
# Duration: ~5-10 minutes

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ­ Axiom Monitoring Package - Post-Deploy Runtime Tests"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Configuration
BACKEND_URL="${BACKEND_URL:-http://localhost:8080}"
PROMETHEUS_URL="${PROMETHEUS_URL:-http://localhost:9090}"
GRAFANA_URL="${GRAFANA_URL:-http://localhost:3000}"
GRAFANA_USER="${GRAFANA_USER:-admin}"
GRAFANA_PASSWORD="${GRAFANA_PASSWORD:-admin}"

TOTAL_TESTS=0
PASSED_TESTS=0
FAILED_TESTS=0
SKIPPED_TESTS=0

# Test result tracking
test_pass() {
  TOTAL_TESTS=$((TOTAL_TESTS + 1))
  PASSED_TESTS=$((PASSED_TESTS + 1))
  echo -e "${GREEN}âœ… PASS${NC}: $1"
}

test_fail() {
  TOTAL_TESTS=$((TOTAL_TESTS + 1))
  FAILED_TESTS=$((FAILED_TESTS + 1))
  echo -e "${RED}âŒ FAIL${NC}: $1"
  echo -e "${RED}   Error: $2${NC}"
}

test_skip() {
  TOTAL_TESTS=$((TOTAL_TESTS + 1))
  SKIPPED_TESTS=$((SKIPPED_TESTS + 1))
  echo -e "${YELLOW}â­ï¸  SKIP${NC}: $1"
  echo -e "${YELLOW}   Reason: $2${NC}"
}

test_info() {
  echo -e "${BLUE}â„¹ï¸  INFO${NC}: $1"
}

wait_for_metric() {
  local metric_name="$1"
  local timeout="${2:-30}"
  local interval=2
  local elapsed=0
  
  echo -e "${PURPLE}â³ Waiting for metric '$metric_name' (timeout: ${timeout}s)${NC}"
  
  while [ $elapsed -lt $timeout ]; do
    if curl -s "${PROMETHEUS_URL}/api/v1/query?query=${metric_name}" | jq -e '.data.result | length > 0' > /dev/null 2>&1; then
      echo -e "${GREEN}   Metric available after ${elapsed}s${NC}"
      return 0
    fi
    sleep $interval
    elapsed=$((elapsed + interval))
  done
  
  echo -e "${RED}   Timeout waiting for metric${NC}"
  return 1
}

# ============================================================================
# PREREQUISITE CHECKS
# ============================================================================
echo -e "\n${BLUE}ğŸ” Prerequisite Checks${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Check if services are running
if ! curl -s -f "${BACKEND_URL}/actuator/health" > /dev/null; then
  echo -e "${RED}âŒ Backend not running at $BACKEND_URL${NC}"
  echo "Please start services: make up or docker compose up"
  exit 1
fi
test_info "Backend running: $BACKEND_URL"

if ! curl -s -f "${PROMETHEUS_URL}/-/healthy" > /dev/null; then
  echo -e "${RED}âŒ Prometheus not running at $PROMETHEUS_URL${NC}"
  exit 1
fi
test_info "Prometheus running: $PROMETHEUS_URL"

if ! curl -s -f "${GRAFANA_URL}/api/health" > /dev/null; then
  echo -e "${RED}âŒ Grafana not running at $GRAFANA_URL${NC}"
  exit 1
fi
test_info "Grafana running: $GRAFANA_URL"

# Check if jq is installed
if ! command -v jq &> /dev/null; then
  echo -e "${RED}âŒ jq not installed${NC}"
  echo "Install: brew install jq (macOS)"
  exit 1
fi

# ============================================================================
# TEST 1: Prometheus Recording Rules
# ============================================================================
echo -e "\n${BLUE}ğŸ“ TEST 1: Prometheus Recording Rules${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# Check if rules are loaded
RULES_RESPONSE=$(curl -s "${PROMETHEUS_URL}/api/v1/rules")
AXIOM_RULES_COUNT=$(echo "$RULES_RESPONSE" | jq '[.data.groups[] | select(.name | startswith("axiom_"))] | length')

if [ "$AXIOM_RULES_COUNT" -gt 0 ]; then
  test_pass "Recording rules loaded in Prometheus ($AXIOM_RULES_COUNT groups)"
else
  test_fail "Recording rules" "No Axiom rule groups found in Prometheus"
fi

# Verify specific recording rules exist
RECORDING_RULES=(
  "app:http_requests:error_rate5m"
  "app:http_requests:latency_p95"
  "app:slo:availability5m"
  "app:kafka:consumer_lag_total"
  "app:db:pool_saturation"
  "app:redis:hit_rate"
  "app:jvm:heap_utilization"
)

test_info "Checking key recording rules..."
for rule_name in "${RECORDING_RULES[@]}"; do
  if wait_for_metric "$rule_name" 10; then
    test_pass "Recording rule exists: $rule_name"
  else
    test_fail "Recording rule missing" "$rule_name not found in Prometheus"
  fi
done

# ============================================================================
# TEST 2: Prometheus Alert Rules
# ============================================================================
echo -e "\n${BLUE}ğŸš¨ TEST 2: Prometheus Alert Rules${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

ALERTS_RESPONSE=$(curl -s "${PROMETHEUS_URL}/api/v1/rules")
AXIOM_ALERTS_COUNT=$(echo "$ALERTS_RESPONSE" | jq '[.data.groups[] | select(.name | contains("axiom_")) | .rules[] | select(.type == "alerting")] | length')

if [ "$AXIOM_ALERTS_COUNT" -gt 0 ]; then
  test_pass "Alert rules loaded in Prometheus ($AXIOM_ALERTS_COUNT alerts)"
else
  test_fail "Alert rules" "No Axiom alerts found in Prometheus"
fi

# Check that alerts have required annotations
ALERTS_WITH_RUNBOOKS=$(echo "$ALERTS_RESPONSE" | jq '[.data.groups[] | select(.name | contains("axiom_")) | .rules[] | select(.type == "alerting" and .annotations.runbook_url)] | length')
ALERTS_WITH_DASHBOARDS=$(echo "$ALERTS_RESPONSE" | jq '[.data.groups[] | select(.name | contains("axiom_")) | .rules[] | select(.type == "alerting" and .annotations.dashboard_url)] | length')

if [ "$ALERTS_WITH_RUNBOOKS" -eq "$AXIOM_ALERTS_COUNT" ]; then
  test_pass "All alerts have runbook URLs ($ALERTS_WITH_RUNBOOKS/$AXIOM_ALERTS_COUNT)"
else
  test_warn "Some alerts missing runbook URLs ($ALERTS_WITH_RUNBOOKS/$AXIOM_ALERTS_COUNT)"
fi

if [ "$ALERTS_WITH_DASHBOARDS" -gt 0 ]; then
  test_pass "Alerts have dashboard URLs ($ALERTS_WITH_DASHBOARDS/$AXIOM_ALERTS_COUNT)"
else
  test_warn "No alerts have dashboard URLs"
fi

# ============================================================================
# TEST 3: Grafana Dashboards Availability
# ============================================================================
echo -e "\n${BLUE}ğŸ“Š TEST 3: Grafana Dashboards${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

EXPECTED_DASHBOARDS=(
  "axiom_sys_overview:System Overview"
  "axiom_adv_runtime:JVM Runtime"
  "axiom_adv_db:Database Operations"
  "axiom_adv_redis:Redis Cache"
  "axiom_kafka_lag:Kafka Consumer Lag"
  "axiom_security:Security & Auth"
  "axiom_audit:Audit & Governance"
)

test_info "Checking Grafana dashboard availability..."

for dashboard_entry in "${EXPECTED_DASHBOARDS[@]}"; do
  IFS=':' read -r uid title <<< "$dashboard_entry"
  
  DASHBOARD_RESPONSE=$(curl -s -u "$GRAFANA_USER:$GRAFANA_PASSWORD" \
    "${GRAFANA_URL}/api/dashboards/uid/${uid}")
  
  if echo "$DASHBOARD_RESPONSE" | jq -e '.dashboard.uid' > /dev/null 2>&1; then
    PANEL_COUNT=$(echo "$DASHBOARD_RESPONSE" | jq '.dashboard.panels | length')
    test_pass "Dashboard available: $title (uid=$uid, panels=$PANEL_COUNT)"
  else
    test_fail "Dashboard not found" "$title (uid=$uid)"
  fi
done

# ============================================================================
# TEST 4: Simulate Error Spike & Verify SLO Metrics
# ============================================================================
echo -e "\n${BLUE}âš ï¸  TEST 4: Error Spike Simulation${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

test_info "Generating 5xx errors to trigger SLO burn rate alert..."

# Generate 10 bad requests (should return 500 or similar error)
for i in {1..10}; do
  curl -s "${BACKEND_URL}/api/trigger-error" > /dev/null 2>&1 || true
  sleep 0.5
done

test_info "Waiting for metrics to update (30s)..."
sleep 30

# Check error rate metric
ERROR_RATE=$(curl -s "${PROMETHEUS_URL}/api/v1/query?query=app:http_requests:error_rate5m" | \
  jq -r '.data.result[0].value[1] // "0"')

if [ "$(echo "$ERROR_RATE > 0" | bc -l)" -eq 1 ]; then
  test_pass "Error rate metric updated: ${ERROR_RATE}%"
else
  test_warn "Error rate metric not updated (may need more traffic)"
fi

# Check SLO availability
AVAILABILITY=$(curl -s "${PROMETHEUS_URL}/api/v1/query?query=app:slo:availability5m" | \
  jq -r '.data.result[0].value[1] // "100"')

test_info "Current SLO availability: ${AVAILABILITY}%"

if [ "$(echo "$AVAILABILITY < 100" | bc -l)" -eq 1 ]; then
  test_pass "SLO availability affected by errors: ${AVAILABILITY}%"
else
  test_warn "SLO availability still 100% (may need more errors)"
fi

# ============================================================================
# TEST 5: Audit Event Logging
# ============================================================================
echo -e "\n${BLUE}ğŸ“ TEST 5: Audit Event Logging${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

test_info "Simulating audit events (CREATE/UPDATE/DELETE)..."

# This requires backend API - skip if endpoint not available
if curl -s -f "${BACKEND_URL}/actuator" > /dev/null; then
  # Try to trigger audit events via API calls
  # Note: Replace with actual audit-triggering endpoints
  
  test_info "Audit events simulation requires authenticated API calls"
  test_skip "Audit event simulation" "Requires authenticated session"
else
  test_skip "Audit event simulation" "Backend actuator not accessible"
fi

# Check if audit metrics exist (they may exist from previous usage)
AUDIT_METRICS=$(curl -s "${PROMETHEUS_URL}/api/v1/label/__name__/values" | \
  jq -r '.data[] | select(. | contains("audit"))' | wc -l)

if [ "$AUDIT_METRICS" -gt 0 ]; then
  test_info "Audit metrics exist: $AUDIT_METRICS metrics"
else
  test_warn "No audit metrics found (may require actual usage)"
fi

# ============================================================================
# TEST 6: Frontend Integration
# ============================================================================
echo -e "\n${BLUE}ğŸŒ TEST 6: Frontend Monitoring Page${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

test_info "Checking frontend monitoring page availability..."

# Try to access monitoring page via nginx (if running)
FRONTEND_URL="${FRONTEND_URL:-http://localhost}"

if curl -s "${FRONTEND_URL}/core-admin/axiom-monitoring" | grep -q "axiom_sys_overview"; then
  test_pass "Frontend monitoring page accessible"
else
  test_skip "Frontend monitoring page" "Page not accessible or requires authentication"
fi

# ============================================================================
# TEST 7: Dashboard Rendering Test
# ============================================================================
echo -e "\n${BLUE}ğŸ–¼ï¸  TEST 7: Dashboard Rendering${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

test_info "Testing dashboard rendering via Grafana API..."

# Render axiom_sys_overview dashboard (snapshot API)
RENDER_RESPONSE=$(curl -s -u "$GRAFANA_USER:$GRAFANA_PASSWORD" \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{
    "dashboard": "axiom_sys_overview",
    "panelId": 1,
    "width": 1000,
    "height": 500
  }' \
  "${GRAFANA_URL}/api/snapshots" 2>&1)

if echo "$RENDER_RESPONSE" | grep -q "url"; then
  test_pass "Dashboard rendering successful (snapshot API)"
else
  # Rendering may fail if no data - check if dashboard at least loads
  DASHBOARD_LOAD=$(curl -s -u "$GRAFANA_USER:$GRAFANA_PASSWORD" \
    "${GRAFANA_URL}/api/dashboards/uid/axiom_sys_overview")
  
  if echo "$DASHBOARD_LOAD" | jq -e '.dashboard.uid' > /dev/null 2>&1; then
    test_pass "Dashboard loads successfully (data may be empty)"
  else
    test_fail "Dashboard rendering" "Could not load or render dashboard"
  fi
fi

# ============================================================================
# SUMMARY
# ============================================================================
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo -e "${BLUE}ğŸ“Š Test Summary${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "Total Tests:  $TOTAL_TESTS"
echo -e "${GREEN}Passed:       $PASSED_TESTS${NC}"
echo -e "${YELLOW}Skipped:      $SKIPPED_TESTS${NC}"

if [ $FAILED_TESTS -gt 0 ]; then
  echo -e "${RED}Failed:       $FAILED_TESTS${NC}"
  echo ""
  echo -e "${RED}âŒ POST-DEPLOY TESTS FAILED${NC}"
  echo "Some runtime tests failed. Review errors above."
  exit 1
else
  echo -e "${RED}Failed:       $FAILED_TESTS${NC}"
  echo ""
  
  if [ $SKIPPED_TESTS -gt 0 ]; then
    echo -e "${YELLOW}âš ï¸  POST-DEPLOY TESTS COMPLETED WITH SKIPS${NC}"
    echo "Some tests were skipped. This is normal for automated testing."
    echo "Manual verification recommended for skipped tests."
  else
    echo -e "${GREEN}âœ… ALL POST-DEPLOY TESTS PASSED${NC}"
    echo "Monitoring package is fully functional!"
  fi
  
  exit 0
fi
