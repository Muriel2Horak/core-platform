# ğŸ” ANALÃZA: CDC Pipeline & Tenant Management

**Datum:** 2024-10-13  
**Autor:** GitHub Copilot + Martin HorÃ¡k  
**Status:** ğŸ”´ ProblÃ©my identifikovÃ¡ny

---

## ğŸ“‹ ODPOVÄšDI NA OTÃZKY

### 1ï¸âƒ£ **Tenant Management - Metamodel Integration**

#### âŒ ZJIÅ TÄšNÃ: Tenant NENÃ pÅ™es Metamodel

**SouÄasnÃ½ stav:**
```java
// TenantManagementController.java
@PostMapping
public ResponseEntity<Map<String, Object>> createTenant(@Valid @RequestBody CreateTenantRequest request) {
    keycloakRealmManagementService.createTenant(request.getKey(), request.getDisplayName());
    // PÅ™Ã­mÃ¡ API logika, NE metamodel
}
```

**Flow:**
```
POST /api/admin/tenants
  â†“
TenantManagementController.createTenant()
  â†“
KeycloakRealmManagementService.createTenant()
  â†“
1. Load template
2. Create Keycloak realm
3. TenantService.createTenantRegistryWithRealmId() â† Direct JPA
4. GrafanaProvisioningService.provisionTenant() â† Direct service call
```

**DÅ¯sledky:**
- âŒ Tenant operace NEJSOU trackovÃ¡ny v Kafka
- âŒ Tenant zmÄ›ny NEJSOU dostupnÃ© pro streaming
- âŒ Tenant CRUD NEMÃ lifecycle eventy
- âŒ Å½Ã¡dnÃ¡ integrace s metamodel workflow

**Å˜eÅ¡enÃ­:**
- âš ï¸ **NE pro tenanty** - metamodel je pro business entity (users, documents, atd.)
- âœ… **SprÃ¡vnÃ©:** Tenant je infrastrukturnÃ­ entita, direct API je OK
- ğŸ’¡ **Alternativa:** VytvoÅ™it `TenantEntity` metamodel pokud chceme tracking

---

### 2ï¸âƒ£ **Flyway Migration V3 vs V1**

#### âœ… DOPORUÄŒENÃ: PÅ™esunout do V1__init.sql

**AktuÃ¡lnÃ­ stav:**
- V1__init.sql - hlavnÃ­ schema (users, tenants, atd.)
- V2__init_keycloak_cdc.sql - change_events table
- **V3__grafana_tenant_bindings.sql** - novÃ¡ tabulka

**ProÄ pÅ™esunout do V1:**
```sql
-- V1__init.sql je 36KB, obsahuje vÅ¡echny core tabulky
-- V3 je jenom 1KB (grafana_tenant_bindings)
-- KdyÅ¾ rebuildujeÅ¡ prostÅ™edÃ­ od zaÄÃ¡tku, V3 stejnÄ› pobÄ›Å¾Ã­
```

**Akce:**
1. Smazat V3__grafana_tenant_bindings.sql
2. PÅ™idat do V1__init.sql na konec:

```sql
-- ====== GRAFANA TENANT BINDINGS (MONITORING) ======

CREATE TABLE grafana_tenant_bindings (
    id BIGSERIAL PRIMARY KEY,
    tenant_id VARCHAR(100) NOT NULL UNIQUE,
    grafana_org_id BIGINT NOT NULL,
    service_account_id BIGINT NOT NULL,
    service_account_name VARCHAR(200) NOT NULL,
    service_account_token VARCHAR(500) NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_grafana_tenant_bindings_tenant_id ON grafana_tenant_bindings(tenant_id);
CREATE INDEX idx_grafana_tenant_bindings_grafana_org_id ON grafana_tenant_bindings(grafana_org_id);

COMMENT ON TABLE grafana_tenant_bindings IS 'Mapping between tenants and Grafana organizations with service account tokens for monitoring';
```

---

### 3ï¸âƒ£ **Admin Tenant - Service Account pÅ™i Bootstrap**

#### ğŸ¤” PROBLÃ‰M: Kdy provisionovat admin tenant?

**SouÄasnÃ½ stav:**
```java
// KeycloakInitializationService.java - Bootstrap
@PostConstruct
public void init() {
    if (firstRun) {
        initializeMasterRealmClient();
        createAdminRealm();
        tenantService.createTenantRegistry("admin"); // â† Admin tenant
        // âŒ ALE: GrafanaProvisioningService.provisionTenant() se NEVOLÃ!
    }
}
```

**ProÄ provisioning NEVOLÃ:**
- Admin tenant se vytvÃ¡Å™Ã­ v `@PostConstruct` (bootstrap)
- `KeycloakRealmManagementService.createTenant()` se pouÅ¾Ã­vÃ¡ jen pro novÃ© tenanty pÅ™es API
- Bootstrap pouÅ¾Ã­vÃ¡ pÅ™Ã­mÃ½ `TenantService.createTenantRegistry()`

**DÅ¯sledky:**
- âŒ Admin tenant NEMÃ Grafana org/SA/token
- âŒ Dashboard pro admin tenant bude mÃ­t spinner
- âŒ Manual provisioning nutnÃ½

**Å˜eÅ¡enÃ­ - Varianta A: Auto-provision pÅ™i bootstrap**
```java
// KeycloakInitializationService.java
@PostConstruct
public void init() {
    if (firstRun) {
        initializeMasterRealmClient();
        createAdminRealm();
        
        // Create tenant registry
        tenantService.createTenantRegistry("admin");
        
        // ğŸš€ AUTO-PROVISION GRAFANA
        try {
            grafanaProvisioningService.provisionTenant("admin");
            log.info("âœ… Grafana provisioned for admin tenant");
        } catch (Exception e) {
            log.error("âš ï¸ Failed to provision Grafana for admin tenant (manual setup required)", e);
        }
    }
}
```

**Å˜eÅ¡enÃ­ - Varianta B: Manual provision pÅ™i prvnÃ­m spuÅ¡tÄ›nÃ­**
```bash
# Po build prostÅ™edÃ­ zavolat:
curl -X POST http://localhost:8080/api/admin/grafana/provision/admin \
  -H "Authorization: Bearer $ADMIN_TOKEN"
```

---

### 4ï¸âƒ£ **Service Account Per Tenant - Je To NutnÃ©?**

#### âœ… ANO, Je To SprÃ¡vnÃ½ Design

**DÅ¯vody:**

**1. Security Isolation:**
```
Tenant A â†’ SA Token A â†’ Org 1 â†’ Only Tenant A logs
Tenant B â†’ SA Token B â†’ Org 2 â†’ Only Tenant B logs
```
- âœ… Token leak v Tenant A neohrozÃ­ Tenant B
- âœ… KaÅ¾dÃ½ tenant mÃ¡ vlastnÃ­ scope/permissions

**2. Monitoring Separation:**
```
Grafana Org 1 (Tenant A):
  - Dashboards: Tenant A specific
  - Data Sources: Filtered by tenant_id=A
  - Alerts: Tenant A only

Grafana Org 2 (Tenant B):
  - Dashboards: Tenant B specific
  - Data Sources: Filtered by tenant_id=B
  - Alerts: Tenant B only
```

**3. Multi-tenancy Best Practices:**
- âœ… **SPRÃVNÃ‰:** KaÅ¾dÃ½ tenant = vlastnÃ­ org + SA
- âŒ **Å PATNÃ‰:** Shared SA pro vÅ¡echny tenanty (security risk)

**AlternativnÃ­ Architektury:**

**Option 1: Single SA with Org Switching (NEDOPORUÄŒENO)**
```java
// Teoreticky moÅ¾nÃ©, ale sloÅ¾itÃ©
grafanaClient.switchOrg(tenantOrgId);
grafanaClient.query(datasource, query);
// âš ï¸ Race conditions, sloÅ¾itÃ½ error handling
```

**Option 2: Master Token per Tenant Org (MOÅ½NÃ‰)**
```
Tenant A â†’ Master Token â†’ API call with X-Grafana-Org-Id: 1
Tenant B â†’ Master Token â†’ API call with X-Grafana-Org-Id: 2
```
- âš ï¸ Master token mÃ¡ pÅ™Ã­liÅ¡ velkÃ© permissions
- âš ï¸ Token leak = pÅ™Ã­stup ke vÅ¡em tenantÅ¯m

**DoporuÄenÃ­:** âœ… **Ponechat 1 SA per tenant** (souÄasnÃ½ design je sprÃ¡vnÃ½)

---

### 5ï¸âƒ£ **CDC Pipeline - User Directory Synchronizace Nefunguje**

#### ğŸ”´ KRITICKÃ PROBLÃ‰M: Data neteÄou z Keycloaku do User Directory

**Symptoms:**
- âœ… ZmÄ›ny v Keycloak admin console probÄ›hnou
- âŒ UserDirectory tabulka zÅ¯stÃ¡vÃ¡ prÃ¡zdnÃ¡/nezmÄ›nÄ›nÃ¡
- âŒ Å½Ã¡dnÃ© chyby v backend logu

**CDC Pipeline Flow (TeoretickÃ½):**
```
Keycloak DB Change
  â†“
Postgres Trigger (INSERT INTO change_events)
  â†“
ChangeEventPollingService.pollAndProcessEvents() (@Scheduled)
  â†“
KeycloakEventProjectionService.processCdcEvent()
  â†“
KeycloakSyncService.syncUserFromKeycloak()
  â†“
MetamodelCrudService.createOrUpdate("User", userData)
  â†“
user_directory table updated
```

#### ğŸ” DIAGNOSTIC CHECKLIST

**Step 1: Verify Trigger Exists**
```sql
-- Zkontrolovat triggery v Keycloak DB
SELECT trigger_name, event_manipulation, event_object_table 
FROM information_schema.triggers 
WHERE trigger_schema = 'public' 
  AND event_object_table IN ('user_entity', 'user_role_mapping', 'user_group_membership');
```

**Expected Output:**
```
trigger_name              | event_manipulation | event_object_table
--------------------------+--------------------+---------------------
user_entity_cdc_trigger   | INSERT             | user_entity
user_entity_cdc_trigger   | UPDATE             | user_entity
user_entity_cdc_trigger   | DELETE             | user_entity
role_mapping_cdc_trigger  | INSERT             | user_role_mapping
...
```

**Step 2: Verify Change Events Are Being Written**
```sql
-- V Keycloak DB
SELECT COUNT(*) FROM change_events;
SELECT * FROM change_events ORDER BY timestamp DESC LIMIT 10;
```

**Expected:** MÄ›ly by tam bÃ½t Å™Ã¡dky po kaÅ¾dÃ© zmÄ›nÄ› v Keycloaku

**Step 3: Verify Polling Service Is Running**
```bash
docker logs backend 2>&1 | grep -i "Change Event Polling"
```

**Expected Output:**
```
ğŸ”„ Change Event Polling Service initialized
   - Batch size: 100
   - Flush interval: 10 seconds
   - Listener enabled: true
ğŸ“¨ Found 5 unprocessed change events, processing...
```

**Step 4: Check Configuration**
```properties
# application.properties
app.change-events.listener-enabled=true   # â† MUST BE TRUE
app.change-events.batch-size=100
app.change-events.flush-interval-seconds=10
```

**Step 5: Verify Keycloak DataSource Connection**
```java
// KeycloakDataSourceConfig.java
@Bean(name = "keycloakDataSource")
public DataSource keycloakDataSource() {
    // url=jdbc:postgresql://db:5432/keycloak
    // username=keycloak
    // password=keycloak
}
```

**Step 6: Check Realm ID Mapping**
```bash
docker logs backend 2>&1 | grep "realm_id"
```

**Expected:** MÄ›lo by bÃ½t vidÄ›t mapovÃ¡nÃ­ `realm_id â†’ tenant_key`

---

## ğŸ› PRAVDÄšPODOBNÃ‰ PÅ˜ÃÄŒINY PROBLÃ‰MU

### PÅ™Ã­Äina #1: Triggery neexistujÃ­ v Keycloak DB
```sql
-- V2__init_keycloak_cdc.sql vytvoÅ™il change_events table
-- ALE: Triggery na user_entity, user_role_mapping CHYBÃ!
```

**Fix:**
```sql
-- PÅ™idat do V2__init_keycloak_cdc.sql

-- Trigger function
CREATE OR REPLACE FUNCTION notify_change_event()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO change_events (
        event_type,
        entity_id,
        realm_id,
        timestamp,
        old_data,
        new_data,
        processed
    ) VALUES (
        TG_OP, -- INSERT/UPDATE/DELETE
        COALESCE(NEW.id, OLD.id),
        COALESCE(NEW.realm_id, OLD.realm_id),
        NOW(),
        to_jsonb(OLD),
        to_jsonb(NEW),
        false
    );
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Triggers na Keycloak tables
CREATE TRIGGER user_entity_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON user_entity
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER user_role_mapping_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON user_role_mapping
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER user_group_membership_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON user_group_membership
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER keycloak_role_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON keycloak_role
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER keycloak_group_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON keycloak_group
FOR EACH ROW EXECUTE FUNCTION notify_change_event();
```

### PÅ™Ã­Äina #2: Polling Service Disabled
```properties
# .env nebo application.properties
app.change-events.listener-enabled=false  # â† PROBLÃ‰M!
```

**Fix:**
```bash
# .env
APP_CHANGE_EVENTS_LISTENER_ENABLED=true
```

### PÅ™Ã­Äina #3: Keycloak DataSource NenÃ­ SprÃ¡vnÄ› NakonfigurovÃ¡n
```bash
# Zkontrolovat credentials
grep KEYCLOAK_DB .env

# MUSÃ bÃ½t:
KEYCLOAK_DB_USERNAME=keycloak
KEYCLOAK_DB_PASSWORD=keycloak
```

### PÅ™Ã­Äina #4: Realm ID Mapping SelhÃ¡vÃ¡
```java
// ChangeEventPollingService.java
private String mapRealmIdToTenantKey(String realmId) {
    // âŒ Pokud realmId neodpovÃ­dÃ¡ Å¾Ã¡dnÃ©mu tenantu, vrÃ¡tÃ­ null
    // âŒ Event se oznaÄÃ­ jako processed ale nic se neudÄ›lÃ¡
}
```

**Debug:**
```sql
-- Zjistit realm_id z Keycloaku
SELECT id, name FROM realm;

-- Porovnat s tenants table v core DB
SELECT id, key, keycloak_realm_id FROM tenants;
```

---

## ğŸ”§ ACTION PLAN - FIX CDC SYNCHRONIZATION

### Step 1: Verify Database Triggers
```bash
docker exec -it db psql -U keycloak -d keycloak -c "
SELECT 
    t.trigger_name, 
    t.event_manipulation, 
    t.event_object_table,
    t.action_statement
FROM information_schema.triggers t
WHERE t.trigger_schema = 'public'
  AND t.event_object_table IN ('user_entity', 'user_role_mapping', 'keycloak_role', 'keycloak_group')
ORDER BY t.event_object_table, t.trigger_name;
"
```

**Expected:** MÄ›ly by bÃ½t triggery (pokud NE â†’ pokraÄuj Step 2)

### Step 2: Add Missing Triggers
```bash
# VytvoÅ™it SQL soubor s triggery
cat > /tmp/add_cdc_triggers.sql << 'EOF'
-- Trigger function for change_events
CREATE OR REPLACE FUNCTION notify_change_event()
RETURNS TRIGGER AS $$
DECLARE
    v_event_type TEXT;
BEGIN
    -- Map trigger operation to event type
    v_event_type := CASE TG_TABLE_NAME
        WHEN 'user_entity' THEN 
            CASE TG_OP
                WHEN 'INSERT' THEN 'USER_CREATED'
                WHEN 'UPDATE' THEN 'USER_UPDATED'
                WHEN 'DELETE' THEN 'USER_DELETED'
            END
        WHEN 'user_role_mapping' THEN 'USER_ROLE_MAPPING_CHANGED'
        WHEN 'user_group_membership' THEN 'USER_GROUP_MEMBERSHIP_CHANGED'
        WHEN 'keycloak_role' THEN
            CASE TG_OP
                WHEN 'INSERT' THEN 'ROLE_CREATED'
                WHEN 'UPDATE' THEN 'ROLE_UPDATED'
                WHEN 'DELETE' THEN 'ROLE_DELETED'
            END
        WHEN 'keycloak_group' THEN
            CASE TG_OP
                WHEN 'INSERT' THEN 'GROUP_CREATED'
                WHEN 'UPDATE' THEN 'GROUP_UPDATED'
                WHEN 'DELETE' THEN 'GROUP_DELETED'
            END
    END;

    INSERT INTO change_events (
        event_type,
        entity_id,
        realm_id,
        timestamp,
        old_data,
        new_data,
        processed
    ) VALUES (
        v_event_type,
        COALESCE(NEW.id, OLD.id),
        COALESCE(NEW.realm_id, OLD.realm_id),
        NOW(),
        CASE WHEN TG_OP != 'INSERT' THEN to_jsonb(OLD) ELSE NULL END,
        CASE WHEN TG_OP != 'DELETE' THEN to_jsonb(NEW) ELSE NULL END,
        false
    );

    RETURN COALESCE(NEW, OLD);
END;
$$ LANGUAGE plpgsql;

-- Drop existing triggers if any
DROP TRIGGER IF EXISTS user_entity_cdc_trigger ON user_entity;
DROP TRIGGER IF EXISTS user_role_mapping_cdc_trigger ON user_role_mapping;
DROP TRIGGER IF EXISTS user_group_membership_cdc_trigger ON user_group_membership;
DROP TRIGGER IF EXISTS keycloak_role_cdc_trigger ON keycloak_role;
DROP TRIGGER IF EXISTS keycloak_group_cdc_trigger ON keycloak_group;

-- Create triggers
CREATE TRIGGER user_entity_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON user_entity
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER user_role_mapping_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON user_role_mapping
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER user_group_membership_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON user_group_membership
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER keycloak_role_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON keycloak_role
FOR EACH ROW EXECUTE FUNCTION notify_change_event();

CREATE TRIGGER keycloak_group_cdc_trigger
AFTER INSERT OR UPDATE OR DELETE ON keycloak_group
FOR EACH ROW EXECUTE FUNCTION notify_change_event();
EOF

# Aplikovat do Keycloak DB
docker exec -i db psql -U keycloak -d keycloak < /tmp/add_cdc_triggers.sql
```

### Step 3: Verify Polling Service Config
```bash
# Zkontrolovat application.properties
grep -E "change-events|listener-enabled" backend/src/main/resources/application.properties

# MÄ›lo by bÃ½t:
# app.change-events.listener-enabled=true (nebo chybÃ­ = default true)
```

### Step 4: Test CDC Flow
```bash
# 1. Restart backend (aby naÄetl config)
docker restart backend

# 2. Sledovat logy
docker logs -f backend | grep -E "Change Event|CDC|polling"

# 3. UdÄ›lat zmÄ›nu v Keycloak (pÅ™es admin console)
#    - VytvoÅ™it novÃ©ho usera
#    - Nebo zmÄ›nit existujÃ­cÃ­ho

# 4. MÄ›lo by se objevit:
# ğŸ“¨ Found X unprocessed change events, processing...
# ğŸ”„ Processing CDC event: type=USER_CREATED, entity=xxx, tenant=admin
# âœ… CDC event processed
```

### Step 5: Verify Data in User Directory
```bash
docker exec -it db psql -U core -d core -c "SELECT username, email, first_name, last_name, tenant_id FROM user_directory ORDER BY created_at DESC LIMIT 10;"
```

**Expected:** MÄ›li byste vidÄ›t uÅ¾ivatele, kteÅ™Ã­ byli vytvoÅ™eni/upraveni v Keycloaku

---

## ğŸ“Š DEBUGGING QUERIES

```sql
-- 1. Check change_events table
SELECT COUNT(*) as total,
       SUM(CASE WHEN processed THEN 1 ELSE 0 END) as processed,
       SUM(CASE WHEN NOT processed THEN 1 ELSE 0 END) as pending
FROM change_events;

-- 2. Recent unprocessed events
SELECT id, event_type, entity_id, realm_id, timestamp, processed
FROM change_events
WHERE NOT processed
ORDER BY timestamp DESC
LIMIT 10;

-- 3. Recent processed events
SELECT id, event_type, entity_id, realm_id, timestamp
FROM change_events
WHERE processed
ORDER BY processed_at DESC
LIMIT 10;

-- 4. Tenant to realm mapping
SELECT t.key as tenant_key, t.keycloak_realm_id, r.name as realm_name
FROM tenants t
LEFT JOIN realm r ON t.keycloak_realm_id = r.id;

-- 5. User directory status
SELECT tenant_id, COUNT(*) as user_count
FROM user_directory
GROUP BY tenant_id;
```

---

## âœ… CHECKLIST - Po Opravu

- [ ] Triggery existujÃ­ v Keycloak DB
- [ ] `change_events` table obsahuje novÃ© zÃ¡znamy po zmÄ›nÄ› v Keycloaku
- [ ] Polling service je enabled (`app.change-events.listener-enabled=true`)
- [ ] Backend logy ukazujÃ­ "Found X unprocessed events"
- [ ] Backend logy ukazujÃ­ "CDC event processed"
- [ ] `user_directory` table obsahuje data po synchronizaci
- [ ] Realm ID â†’ tenant_key mapping funguje
- [ ] Å½Ã¡dnÃ© error zprÃ¡vy v backend logu

---

## ğŸ“ SOUHRNNÃ‰ ZÃVÄšRY

1. **Tenant Management:** 
   - âœ… NE pÅ™es metamodel (sprÃ¡vnÄ›)
   - âœ… Direct API je vhodnÃ½ pÅ™Ã­stup
   
2. **Grafana Bindings Migration:**
   - âœ… PÅ™esunout do V1__init.sql
   - â³ Akce: Manual merge do V1
   
3. **Admin Tenant Provisioning:**
   - âŒ ChybÃ­ auto-provisioning pÅ™i bootstrap
   - ğŸ”§ Fix: PÅ™idat `grafanaProvisioningService.provisionTenant("admin")` do `KeycloakInitializationService.init()`
   
4. **Service Account Per Tenant:**
   - âœ… ANO, je to sprÃ¡vnÃ© (security + isolation)
   - âœ… Ponechat souÄasnÃ½ design
   
5. **CDC Pipeline:**
   - ğŸ”´ KRITICKÃ: PravdÄ›podobnÄ› chybÃ­ triggery v Keycloak DB
   - ğŸ”§ Fix: Aplikovat CDC triggery (viz Step 2)
   - ğŸ”§ Verify: Polling service config + realm ID mapping

---

**Next Steps:**
1. OvÄ›Å™it triggery v Keycloak DB
2. PÅ™idat chybÄ›jÃ­cÃ­ triggery
3. Testovat CDC flow
4. Opravit admin tenant provisioning
5. PÅ™esunout V3 do V1
