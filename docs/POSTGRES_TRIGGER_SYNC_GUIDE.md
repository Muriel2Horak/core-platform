# ğŸ”„ PostgreSQL Trigger Synchronization System

## PÅ™ehled

Tento dokument popisuje novÃ½ synchronizaÄnÃ­ systÃ©m pro zmÄ›ny uÅ¾ivatelÅ¯ v Keycloak, kterÃ½ nahrazuje pÅ¯vodnÃ­ SPI webhook implementaci. NovÃ½ systÃ©m pouÅ¾Ã­vÃ¡ PostgreSQL triggers + NOTIFY/LISTEN + inteligentnÃ­ agregaci pro efektivnÄ›jÅ¡Ã­ a spolehlivÄ›jÅ¡Ã­ synchronizaci.

## Architektura

### StarÃ½ systÃ©m (DEPRECATED)
```
Keycloak Event â†’ SPI Webhook â†’ HTTP POST â†’ Backend Endpoint â†’ User Directory
```

### NovÃ½ systÃ©m
```
Keycloak DB Change â†’ PostgreSQL Trigger â†’ NOTIFY â†’ ChangeEventProcessor â†’ Bulk API â†’ User Directory
```

## Komponenty

### 1. PostgreSQL Triggers & Functions

#### Tabulka: `user_change_events`
```sql
CREATE TABLE user_change_events (
    id BIGSERIAL PRIMARY KEY,
    user_id UUID NOT NULL,
    operation TEXT NOT NULL CHECK (operation IN ('INSERT', 'UPDATE', 'DELETE')),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    realm_id TEXT NOT NULL,
    payload JSONB NULL,
    processed BOOLEAN NOT NULL DEFAULT FALSE,
    processed_at TIMESTAMPTZ NULL
);
```

#### Trigger Functions
- `fn_notify_user_entity_change()` - zachycuje zmÄ›ny v `USER_ENTITY` tabulce
- `fn_notify_user_attribute_change()` - zachycuje zmÄ›ny v `USER_ATTRIBUTE` tabulce

#### Triggers
- `trig_user_entity_change` - na `USER_ENTITY` tabulce
- `trig_user_attribute_change` - na `USER_ATTRIBUTE` tabulce

### 2. Backend Components

#### ChangeEventProcessor
- **LISTEN** na kanÃ¡l `user_entity_changed`
- **In-memory buffer** pro pending user IDs s timestampy
- **PeriodickÃ½ flush job** (kaÅ¾dÃ½ch 10 sekund)
- **Fallback job** pro starÃ© nezpracovanÃ© eventy
- **Cleanup job** pro archivaci starÃ½ch eventÅ¯

#### KeycloakUserSyncService
- **Bulk fetch** uÅ¾ivatelÅ¯ z Keycloak API
- **InteligentnÃ­ agregace** - minimalizuje API volÃ¡nÃ­
- **Deduplicace** eventÅ¯ pro stejnÃ½ user_id
- **Bulk oznaÄovÃ¡nÃ­** jako zpracovanÃ©

#### UserChangeEventRepository
- **SKIP LOCKED** queries pro paralelnÃ­ instancovÃ¡nÃ­
- **Bulk operace** pro efektivnÃ­ zpracovÃ¡nÃ­
- **Monitoring queries** pro health checks

## Konfigurace

### Application Properties
```properties
# Interval flushe zmÄ›n (sekundy)
app.change-events.flush-interval-seconds=10

# Velikost batche pro zpracovÃ¡nÃ­
app.change-events.batch-size=100

# Interval fallback jobu (sekundy) 
app.change-events.fallback-interval-seconds=60

# Cron pro cleanup starÃ½ch eventÅ¯
app.change-events.cleanup-cron=0 30 2 * * *

# ZapnutÃ­/vypnutÃ­ LISTEN
app.change-events.listener-enabled=true
```

### Environment Variables
```bash
# Deaktivace starÃ©ho SPI webhook systÃ©mu
KC_EVENTS_LISTENERS=jboss-logging

# NovÃ½ systÃ©m se konfiguruje pÅ™es application.properties
```

## VÃ½hody novÃ©ho systÃ©mu

### 1. InteligentnÃ­ agregace
- **Korelace zmÄ›n**: VÃ­ce zmÄ›n stejnÃ©ho uÅ¾ivatele = jeden sync
- **ÄŒasovÃ© okno**: ZmÄ›ny se agregujÃ­ po dobu `flush-interval-seconds`
- **Minimalizace API volÃ¡nÃ­**: Bulk operace mÃ­sto jednotlivÃ½ch requestÅ¯

### 2. Odolnost
- **Fallback mechanismus**: StarÃ© nezpracovanÃ© eventy se zpracujÃ­ automaticky
- **SKIP LOCKED**: ParalelnÃ­ instance backendu se neblokujÃ­
- **PersistentnÃ­ queue**: Eventy se neztratÃ­ pÅ™i restartu backendu

### 3. Performance
- **Bulk fetch**: VÃ­ce uÅ¾ivatelÅ¯ najednou z Keycloak API
- **Bulk DB operace**: EfektivnÃ­ oznaÄovÃ¡nÃ­ jako zpracovanÃ©
- **Indexy**: OptimalizovanÃ© pro rychlÃ© vyhledÃ¡vÃ¡nÃ­

### 4. Monitoring
- **Health endpoint**: `/api/admin/change-events/health`
- **DetailnÃ­ statistiky**: `/api/admin/change-events/stats`
- **ManuÃ¡lnÃ­ operace**: flush, cleanup pÅ™es API

## âš¡ V4 Optimalizace - RobustnÃ­ systÃ©m

### NovÃ© funkce v V4
- **NOOP update detekce** - triggery ignorujÃ­ zmÄ›ny bez skuteÄnÃ© zmÄ›ny dat
- **MinimalizovanÃ© NOTIFY payloady** - jen user_id mÃ­sto celÃ©ho JSON
- **Batch delete zpracovanÃ½ch eventÅ¯** - efektivnÃ­ ÄiÅ¡tÄ›nÃ­ s progress trackingem  
- **Reconnect logika** - automatickÃ© obnovenÃ­ LISTEN spojenÃ­ po vÃ½padku
- **Buffer overflow protection** - force flush pÅ™i pÅ™ekroÄenÃ­ max-buffer-size
- **KompozitnÃ­ DB indexy** - optimalizovanÃ© pro rychlÃ© queries
- **Monitoring views** - DB views pro rychlÃ© statistiky
- **Enhanced health checks** - detailnÃ­ monitoring s performance metrikami

### NOOP Update Detection
```sql
-- V triggeru: kontrola zmÄ›n pÅ™ed vytvoÅ™enÃ­m eventu
has_relevant_changes := (
    OLD.username IS DISTINCT FROM NEW.username OR
    OLD.email IS DISTINCT FROM NEW.email OR
    OLD.first_name IS DISTINCT FROM NEW.first_name OR
    OLD.last_name IS DISTINCT FROM NEW.last_name OR
    OLD.enabled IS DISTINCT FROM NEW.enabled
);

IF NOT has_relevant_changes THEN
    RETURN NEW; -- Å½Ã¡dnÃ½ event
END IF;
```

### Batch Delete s Progress
```sql
-- MazÃ¡nÃ­ v dÃ¡vkÃ¡ch s pauzami
CREATE OR REPLACE FUNCTION batch_delete_processed_events(
    user_ids_param UUID[],
    before_timestamp_param TIMESTAMPTZ,
    batch_size_param INTEGER DEFAULT 1000
) RETURNS INTEGER
```

### Buffer Overflow Protection
```java
// Force flush pÅ™i pÅ™ekroÄenÃ­ bufferu
if (pendingUserIds.size() >= maxBufferSize) {
    log.info("ğŸš€ Buffer size limit reached ({}), forcing flush", maxBufferSize);
    flushPendingChanges();
}
```

### Reconnect Logic
```java
// RobustnÃ­ reconnect s exponential backoff
while (shouldReconnect && reconnectAttempts < MAX_RECONNECT_ATTEMPTS) {
    try {
        establishListenConnection();
        runListenLoop();
    } catch (Exception e) {
        reconnectAttempts++;
        Thread.sleep(reconnectDelaySeconds * 1000L);
    }
}
```

## âš ï¸ NOTIFY/LISTEN Rizika a limity

### 1. Payload limity
- **Maximum 8000 bytes** pro NOTIFY payload
- **Å˜eÅ¡enÃ­ V4**: PosÃ­lÃ¡me jen user_id (36 znakÅ¯)
- **PÅ¯vodnÃ­ problÃ©m**: CelÃ½ JSON user data (potenciÃ¡lnÄ› >8KB)

### 2. BlokovÃ¡nÃ­ commitÅ¯
```sql
-- RIZIKO: Mnoho NOTIFY v jednÃ© transakci mÅ¯Å¾e blokovat commit
BEGIN;
-- 1000x NOTIFY calls... 
COMMIT; -- MÅ¯Å¾e trvat dlouho!
```

**Å˜eÅ¡enÃ­**:
- Triggery jsou `AFTER` - commit uÅ¾ probÄ›hl
- Jeden NOTIFY per user zmÄ›na (agregace)
- Monitoring poÄtu notifikacÃ­

### 3. ZtracenÃ© notifikace
- **NOTIFY nenÃ­ garantovanÃ© doruÄenÃ­**
- **Connection drop = ztracenÃ© notifikace**

**Å˜eÅ¡enÃ­**:
```java
// Fallback job kaÅ¾dÃ½ch 60 sekund
@Scheduled(fixedDelayString = "60000")
public void processFallbackEvents() {
    LocalDateTime cutoffTime = LocalDateTime.now().minusSeconds(120);
    List<UserChangeEventEntity> oldEvents = 
        eventRepository.findUnprocessedEventsOlderThan(cutoffTime);
    // ... zpracuj bez notifikacÃ­
}
```

### 4. Memory consumption
- **Buffer mÅ¯Å¾e rÅ¯st** pÅ™i vysokÃ© zÃ¡tÄ›Å¾i
- **Connection pooling** - dedikovanÃ© LISTEN spojenÃ­

**Å˜eÅ¡enÃ­**:
```properties
# V4 optimalizace
app.change-events.max-buffer-size=500  # Force flush limit
app.change-events.reconnect-delay-seconds=5
app.change-events.delete-batch-size=1000
```

## ğŸ“Š Enhanced Monitoring V4

### NovÃ© endpointy
```bash
# RozÅ¡Ã­Å™enÃ© health s connection validacÃ­
GET /api/admin/change-events/health

# DB statistics pÅ™es views
GET /api/admin/change-events/db-stats  

# Force reconnect LISTEN spojenÃ­
POST /api/admin/change-events/reconnect

# Batch cleanup s progress
POST /api/admin/change-events/cleanup?daysOld=7&batchSize=1000
```

### Health Response V4
```json
{
  "status": "UP",
  "processor": {
    "listening": true,
    "connectionValid": true,
    "reconnectAttempts": 0,
    "maxReconnectAttempts": 10,
    "pendingUserIds": 5,
    "processingUserIds": 0,
    "maxBufferSize": 500,
    "deleteBatchSize": 1000
  },
  "performance": {
    "avgProcessingTimeSeconds": 0.25,
    "eventsProcessedLastHour": 150
  },
  "database": {
    "unprocessedEvents": 12,
    "totalEvents": 5420,
    "uniqueUnprocessedUsers": 8,
    "uniqueRealms": 3,
    "healthy": true
  }
}
```

### Database Views
```sql
-- CelkovÃ© statistiky
SELECT * FROM v_user_change_events_stats;

-- Breakdown podle realm  
SELECT * FROM v_user_change_events_by_realm 
ORDER BY unprocessed_events DESC;
```

### Grafana Queries V4
```logql
# V4 optimalizace events
{service="backend"} |= "NOOP update" |= "neposÃ­lÃ¡me notifikaci"

# Buffer overflow protection
{service="backend"} |= "Buffer size limit reached" |= "forcing flush"

# Reconnect events
{service="backend"} |= "LISTEN connection failed" |= "attempt"

# Batch delete performance
{service="backend"} |= "deleted" |= "old events"

# Performance metrics
{service="backend"} |= "Processing" |= "events for" |= "users" | 
  regex "Processing (?P<events>\d+) events for (?P<users>\d+) users" |
  rate(1m)
```

### Alerting Rules V4
```yaml
# Critical alerts
- alert: TriggerSyncDown
  expr: trigger_sync_status != 1
  labels:
    severity: critical
  annotations:
    summary: "Trigger sync system is DOWN"

- alert: HighUnprocessedEvents  
  expr: trigger_sync_unprocessed_events > 1000
  labels:
    severity: warning
  annotations:
    summary: "High number of unprocessed events: {{ $value }}"

- alert: ListenConnectionInvalid
  expr: trigger_sync_connection_valid != 1
  labels:
    severity: warning
  annotations:
    summary: "LISTEN connection is not valid"

- alert: HighReconnectAttempts
  expr: trigger_sync_reconnect_attempts > 5
  labels:
    severity: warning
  annotations:
    summary: "High reconnect attempts: {{ $value }}"
```

## ğŸš€ Performance Tuning V4

### Database Level
```sql
-- Composite indexes for performance
CREATE INDEX idx_uce_processed_created_at ON user_change_events(processed, created_at);
CREATE INDEX idx_uce_user_id_created_at ON user_change_events(user_id, created_at);

-- Partitioning pro large scale (budoucnost)
ALTER TABLE user_change_events PARTITION BY RANGE (created_at);
CREATE TABLE user_change_events_2025_01 PARTITION OF user_change_events 
    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');
```

### Application Level
```properties
# Optimized tuning parameters
app.change-events.flush-interval-seconds=5     # ÄŒastÄ›jÅ¡Ã­ flush pro nÃ­zkou latenci
app.change-events.max-buffer-size=1000        # VyÅ¡Å¡Ã­ limit pro high-throughput
app.change-events.batch-size=200              # VÄ›tÅ¡Ã­ batches pro efektivitu
app.change-events.delete-batch-size=5000      # RychlejÅ¡Ã­ cleanup
```

### Connection Pooling
```properties
# Dedicated connection for LISTEN
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.idle-timeout=600000
```

## ğŸ§ª Testing V4 Features

### NOOP Update Test
```bash
# Test Å¾e NOOP update negeneruje eventy
UPDATE public.user_entity 
SET email = 'same@email.com' 
WHERE email = 'same@email.com';  -- StejnÃ¡ hodnota

# OvÄ›Å™: Å¾Ã¡dnÃ© novÃ© eventy
SELECT COUNT(*) FROM user_change_events WHERE processed = false;
```

### Buffer Overflow Test  
```bash
# Simulace buffer overflow
for i in {1..600}; do
  NOTIFY user_entity_changed, '$(uuidgen)';
done

# OvÄ›Å™ force flush
curl /api/admin/change-events/health | jq '.processor.pendingUserIds'  # < 500
```

### Batch Delete Test
```bash
# Test batch delete s progress
curl -X POST "/api/admin/change-events/cleanup?daysOld=1&batchSize=100"

# VÃ½sledek: {"totalDeleted": 250, "batchSize": 100, "status": "success"}
```

### Mass Aggregation Test
```bash
# 100 zmÄ›n stejnÃ©ho uÅ¾ivatele = 1 synchronizace
USER_ID=$(uuidgen)
for i in {1..100}; do
  INSERT INTO user_change_events (...) VALUES ('$USER_ID', ...);
done

NOTIFY user_entity_changed, '$USER_ID';
# VÃ½sledek: 1 sync call, 100 events processed
```

## ğŸ¢ Production Deployment V4

### Pre-deployment Checklist
```bash
# 1. OvÄ›Å™ V4 migraci
docker logs core-backend | grep "V4 Optimization"

# 2. Test vÅ¡ech V4 funkcÃ­
./scripts/test-trigger-sync.sh

# 3. Load test s vysokÃ½m throughputem
# Simuluj 1000 concurrent zmÄ›n uÅ¾ivatelÅ¯

# 4. Verify monitoring
curl /api/admin/change-events/db-stats | jq '.overall'
```

### Capacity Planning V4
```
Expected Load:
- 10,000 users per tenant
- 50 changes per user per day  
- Peak: 500 changes per minute

V4 Performance:
- Buffer: 500 pending users max
- Flush: every 10 seconds
- Throughput: ~50 users/second sustainable
- Batch delete: 5000 events/minute cleanup

Sizing:
- Buffer handles 5-minute peaks (500 users)
- Fallback catches missed notifications (60s interval)
- Database partitioning for >1M events/month
```

### Security Considerations V4
```bash
# Monitoring endpoints pouze admin
@PreAuthorize("hasRole('ADMIN')")

# Database function permissions
GRANT EXECUTE ON FUNCTION batch_delete_processed_events TO core;
GRANT EXECUTE ON FUNCTION bulk_mark_events_processed TO core;

# View permissions  
GRANT SELECT ON v_user_change_events_stats TO monitoring_user;
```

## ğŸ“ˆ Metrics & SLAs V4

### Key Performance Indicators
- **Aggregation ratio**: Events/Syncs (target: >3:1)
- **Processing latency**: Event created â†’ Processed (target: <30s)  
- **Buffer utilization**: Peak pending users (target: <80% of max)
- **Reconnect frequency**: Connections/day (target: <5)
- **NOOP detection rate**: Filtered updates % (target: >20%)

### SLA Targets
- **Availability**: 99.9% (max 43 minutes downtime/month)
- **Event processing**: 95% within 30 seconds
- **No data loss**: 99.99% (fallback job SLA)
- **Buffer overflow**: <1% of flush cycles

## ğŸ”§ Troubleshooting V4

### High Reconnect Rate
```bash
# Check connection health
curl /api/admin/change-events/health | jq '.processor.connectionValid'

# Force reconnect
curl -X POST /api/admin/change-events/reconnect

# Check database connections
SELECT * FROM pg_stat_activity WHERE application_name LIKE '%backend%';
```

### Buffer Overflow Issues
```bash
# Check buffer usage
curl /api/admin/change-events/health | jq '.processor.pendingUserIds'

# Tune buffer size
app.change-events.max-buffer-size=1000  # Increase

# Monitor force flush frequency  
grep "Buffer size limit reached" backend.log
```

### NOOP Detection Not Working
```sql
-- Check trigger functions
SELECT proname FROM pg_proc WHERE proname LIKE '%optimized%';

-- Test NOOP manually
UPDATE user_entity SET email = email WHERE id = 'test-uuid';

-- Should not create events
SELECT COUNT(*) FROM user_change_events WHERE created_at > NOW() - INTERVAL '1 minute';
```

### Batch Delete Performance  
```sql
-- Check processed events accumulation
SELECT 
    processed,
    COUNT(*) as count,
    MIN(processed_at) as oldest_processed
FROM user_change_events 
GROUP BY processed;

-- Manual cleanup if needed
SELECT batch_delete_processed_events(
    ARRAY(SELECT DISTINCT user_id FROM user_change_events WHERE processed = true LIMIT 100),
    NOW() - INTERVAL '1 day',
    1000
);
```

## ğŸ¯ Migration Path: V3 â†’ V4

### Step 1: Deploy V4 Migration
```bash
# Backup before migration
pg_dump core > backup_before_v4.sql

# Deploy with V4 migration
docker-compose up -d --build

# Verify migration
docker logs core-backend | grep "V4 Optimization"
```

### Step 2: Gradual Activation
```properties  
# Start with conservative settings
app.change-events.max-buffer-size=200
app.change-events.delete-batch-size=500
app.change-events.reconnect-delay-seconds=10
```

### Step 3: Performance Tuning
```bash
# Monitor for 24 hours
watch 'curl -s /api/admin/change-events/stats | jq ".performance"'

# Tune based on metrics
# - Increase buffer size if many force flushes
# - Decrease flush interval if latency high  
# - Increase batch sizes if cleanup slow
```

### Step 4: Validation
```bash
# Run enhanced test suite
./scripts/test-trigger-sync.sh

# Load test with production-like data
# - 1000 concurrent user changes
# - NOOP updates mixed in
# - Connection drops simulation
# - Buffer overflow scenarios
```

SystÃ©m je nynÃ­ plnÄ› optimalizovÃ¡n pro produkÄnÃ­ nasazenÃ­ s robustnÃ­ error handling, efektivnÃ­ resource management a komplexnÃ­m monitoringem! ğŸš€

## ğŸ”„ Sequencing instalace - KRITICKÃ‰!

### âŒ IdentifikovanÃ½ problÃ©m
Triggery se pokouÅ¡Ã­ nainstalovat **PÅ˜ED** tÃ­m, neÅ¾ Keycloak vytvoÅ™Ã­ svÃ© tabulky:

```
1. DB container startuje â†’ init skripty
2. Backend startuje â†’ Flyway V4 migrace â†’ pokus o CREATE TRIGGER na neexistujÃ­cÃ­ USER_ENTITY âŒ
3. Keycloak startuje â†’ vytvoÅ™Ã­ USER_ENTITY tabulku
4. Triggery CHYBÃ na Keycloak tabulkÃ¡ch!
```

### âœ… Å˜eÅ¡enÃ­ V4 - Safe Installation

#### 1. **KondicionÃ¡lnÃ­ trigger installation**
```sql
-- V4 migrace nynÃ­ kontroluje existence tabulek
CREATE OR REPLACE FUNCTION install_user_sync_triggers()
RETURNS TEXT AS $$
BEGIN
    IF table_exists('user_entity') THEN
        -- Instaluj trigger
    ELSE
        -- 'USER_ENTITY table not found (will install later)'
    END IF;
END;
$$;
```

#### 2. **Post-startup dokonÄenÃ­**
```java
@EventListener(ApplicationReadyEvent.class)
public void ensureTriggersInstalledAfterStartup() {
    // PoÄkÃ¡ 10 sekund na dokonÄenÃ­ Keycloak startu
    CompletableFuture.runAsync(() -> {
        Thread.sleep(10000);
        
        // ZavolÃ¡ DB funkci pro dokonÄenÃ­ instalace
        jdbcTemplate.queryForObject("SELECT ensure_user_sync_triggers_installed()");
    });
}
```

#### 3. **ManuÃ¡lnÃ­ endpoint pro fallback**
```bash
# Pokud se triggery nepodaÅ™ilo nainstalovat automaticky
curl -X POST /api/admin/change-events/install-triggers
```

### ğŸ“‹ SprÃ¡vnÃ½ deployment flow

#### **Krok 1: Docker Compose Up**
```bash
docker-compose up -d
```

**Co se stane:**
1. âœ… **DB startuje** - zÃ¡kladnÃ­ PostgreSQL
2. âœ… **Backend ÄekÃ¡ na DB health** - pak startuje Spring Boot
3. âœ… **V4 migrace se spustÃ­** - vytvoÅ™Ã­ `user_change_events` tabulku + funkce
4. âš ï¸ **Triggery se NEINSTALUJÃ** - tabulky `USER_ENTITY` neexistujÃ­ jeÅ¡tÄ›
5. âœ… **Keycloak startuje paralelnÄ›** - vytvoÅ™Ã­ svÃ© tabulky
6. âœ… **Backend dokonÄÃ­ startup** - spustÃ­ post-startup trigger installation

#### **Krok 2: AutomatickÃ© dokonÄenÃ­ (10s po startu)**
```
Backend: "ğŸ”§ Checking if user sync triggers need to be installed post-startup..."
DB: "USER_ENTITY table found, installing trigger..."
Backend: "âœ… User sync triggers successfully installed after startup: 2 triggers"
```

#### **Krok 3: Verifikace**
```bash
./scripts/test-trigger-sync.sh
# Test 13 ovÄ›Å™Ã­ Å¾e triggery jsou nainstalovanÃ©
```

### ğŸš¨ Pokud se triggery neinstalujÃ­ automaticky

#### **Debugging:**
```bash
# 1. Zkontroluj logy backendu
docker logs core-backend | grep "trigger"

# 2. Zkontroluj DB stav
docker exec core-db psql -U core -d core -c "
SELECT table_exists('user_entity'), table_exists('user_attribute');
"

# 3. Zkontroluj triggery
docker exec core-db psql -U core -d core -c "
SELECT COUNT(*) FROM information_schema.triggers 
WHERE trigger_name LIKE '%optimized%';
"
```

#### **RuÄnÃ­ oprava:**
```bash
# ManuÃ¡lnÃ­ instalace pÅ™es API
curl -X POST "https://admin.${DOMAIN}/api/admin/change-events/install-triggers"

# Nebo pÅ™Ã­mo v DB
docker exec core-db psql -U core -d core -c "
SELECT install_user_sync_triggers();
"
```

### ğŸ“Š Monitoring sequencingu

#### **Health endpoint ukazuje stav:**
```json
{
  "status": "UP",
  "processor": {
    "listening": true,
    "keycloak_tables_detected": true,
    "triggers_installed": 2
  }
}
```

#### **Install-triggers endpoint:**
```bash
curl -X POST /api/admin/change-events/install-triggers
```

**Response:**
```json
{
  "success": true,
  "keycloak_tables_detected": true,
  "user_entity_exists": true,
  "user_attribute_exists": true,
  "triggers_installed": 2,
  "install_result": "USER_ENTITY trigger installed; USER_ATTRIBUTE trigger installed;",
  "timestamp": 1728000000000
}
```

## ğŸ¯ OdpovÄ›Ä na vaÅ¡i otÃ¡zku

**ANO**, po rebuild od znova **BUDE TO FUNGOVAT**, ale s tÄ›mito Ãºpravami:

### âœ… **Co se stane pÅ™i rebuildu:**

1. **DB container** - vytvoÅ™Ã­ se PostgreSQL s init skripty
2. **Backend container** - spustÃ­ se Spring Boot
3. **V4 Flyway migrace** - vytvoÅ™Ã­ tabulky a funkce, ale **bezpeÄnÄ› pÅ™eskoÄÃ­** triggery pokud Keycloak tabulky neexistujÃ­
4. **Keycloak container** - vytvoÅ™Ã­ svÃ© DB struktury (`USER_ENTITY`, `USER_ATTRIBUTE`, `REALM`)
5. **10 sekund po backend startu** - automaticky doinstaluje triggery na Keycloak tabulky
6. **SystÃ©m je plnÄ› funkÄnÃ­** s V4 optimalizacemi

### âœ… **Kdy se vytvoÅ™Ã­ triggery:**

- **NEJDÅ˜ÃVE** se pokusÃ­ pÅ™i V4 migraci (pravdÄ›podobnÄ› selÅ¾e - Keycloak tabulky neexistujÃ­)
- **10 sekund po backend startu** - automatickÃ© dokonÄenÃ­ instalace  
- **Po vytvoÅ™enÃ­ admin realmu** - triggery uÅ¾ budou urÄitÄ› nainstalovanÃ©

### âœ… **BezpeÄnostnÃ­ zÃ¡ruky:**

- **Å½Ã¡dnÃ© chyby** pÅ™i startu - safe installation ignoruje chybÄ›jÃ­cÃ­ tabulky
- **AutomatickÃ© dokonÄenÃ­** - backend si sÃ¡m doinstaluje triggery kdyÅ¾ jsou tabulky ready
- **ManuÃ¡lnÃ­ fallback** - endpoint pro ruÄnÃ­ instalaci pokud je potÅ™eba
- **Monitoring** - vidÃ­te presnÄ› kdy se triggery nainstalovaly

**SystÃ©m je navrÅ¾en tak, aby fungoval bez ohledu na poÅ™adÃ­ startu Keycloak a backend kontejnerÅ¯!** ğŸš€